{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Asteroids as Hazardous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is about Asteroids - NeoWs.\n",
    "\n",
    "NeoWs (Near Earth Object Web Service) is a RESTful web service for near earth Asteroid information. With NeoWs a user can: search for Asteroids based on their closest approach date to Earth, lookup a specific Asteroid with its NASA JPL small body id, as well as browse the overall data-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to find potential hazardous and non-hazardous asteroids taking into account the features responsible for qualifying an asteroid as hazardous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 4687 entries and provides information on numerous asteroids. Each asteroid is classified as either hazardous or non-hazardous, depending on the value of the ***Hazardous*** column being either ```True``` or ```False```.\n",
    "\n",
    "The table below showcases the original attributes of each entry of the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute                       | Type    |\n",
    "|---------------------------------|----------|\n",
    "| Neo Reference ID                | int64    |\n",
    "| Name                            | int64    |\n",
    "| Absolute Magnitude              | float64  |\n",
    "| Est Dia in KM (min)             | float64  |\n",
    "| Est Dia in KM (max)             | float64  |\n",
    "| Est Dia in M (min)              | float64  |\n",
    "| Est Dia in M (max)              | float64  |\n",
    "| Est Dia in Miles (min)          | float64  |\n",
    "| Est Dia in Miles (max)          | float64  |\n",
    "| Est Dia in Feet (min)           | float64  |\n",
    "| Est Dia in Feet (max)           | float64  |\n",
    "| Close Approach Date             | object   |\n",
    "| Epoch Date Close Approach       | int64    |\n",
    "| Relative Velocity km per sec    | float64  |\n",
    "| Relative Velocity km per hr     | float64  |\n",
    "| Miles per hour                  | float64  |\n",
    "| Miss Dist. (Astronomical)       | float64  |\n",
    "| Miss Dist. (lunar)              | float64  |\n",
    "| Miss Dist. (kilometers)         | float64  |\n",
    "| Miss Dist. (miles)              | float64  |\n",
    "| Orbiting Body                   | object   |\n",
    "| Orbit ID                        | int64    |\n",
    "| Orbit Determination Date        | object   |\n",
    "| Orbit Uncertainty               | int64    |\n",
    "| Minimum Orbit Intersection      | float64  |\n",
    "| Jupiter Tisserand Invariant     | float64  |\n",
    "| Epoch Osculation                | float64  |\n",
    "| Eccentricity                    | float64  |\n",
    "| Semi Major Axis                 | float64  |\n",
    "| Inclination                     | float64  |\n",
    "| Asc Node Longitude              | float64  |\n",
    "| Orbital Period                  | float64  |\n",
    "| Perihelion Distance             | float64  |\n",
    "| Perihelion Arg                  | float64  |\n",
    "| Aphelion Dist                   | float64  |\n",
    "| Perihelion Time                 | float64  |\n",
    "| Mean Anomaly                    | float64  |\n",
    "| Mean Motion                     | float64  |\n",
    "| Equinox                         | object   |\n",
    "| Hazardous                       | bool     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/nasa.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking a look at the table entries, it was clear that the dataset was unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hazardous'].value_counts()\n",
    "\n",
    "df['Hazardous'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['red', 'green'])\n",
    "plt.ylabel('')\n",
    "plt.title('Asteroid Hazard Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, we found columns that were not needed, since they were essentially duplicates with different units of measurement.\n",
    "We also removed identification columns since they didn't provide any useful information for classifying the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    # Remove duplicated columns (same data, different units of measure)\n",
    "    \"Est Dia in KM(min)\",\n",
    "    \"Est Dia in KM(max)\",\n",
    "    \"Est Dia in Miles(min)\",\n",
    "    \"Est Dia in Miles(max)\",\n",
    "    \"Est Dia in Feet(min)\",\n",
    "    \"Est Dia in Feet(max)\",\n",
    "    \"Relative Velocity km per hr\",\n",
    "    \"Miles per hour\",\n",
    "    \"Miss Dist.(Astronomical)\",\n",
    "    \"Miss Dist.(lunar)\",\n",
    "    \"Miss Dist.(miles)\",\n",
    "\n",
    "    # Remove identification columns\n",
    "    \"Neo Reference ID\",\n",
    "    \"Name\",\n",
    "    \"Orbit ID\",\n",
    "    'Close Approach Date',\n",
    "    'Orbit Determination Date',\n",
    "]\n",
    "\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also checked the dataset for any missing or duplicated values and removed data that was not relevant to the problem (columns like ***Equinox*** have a single value for every entry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated values\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Hazardous\"] = label_encoder.fit_transform(df[\"Hazardous\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove categorical data that is not relevant to the problem (has a single value)\n",
    "categorical = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "unique_categorical = [cat for cat in categorical if df[cat].nunique() == 1]\n",
    "unique_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Orbiting Body', 'Equinox'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done the preprocessing, we then proceeded to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sb.heatmap(correlation_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the heatmap in mind we can see that:\n",
    "\n",
    "    - The columns `Est Dia in M(min)` and `Est Dia in M(max)` are highly correlated.\n",
    "    - The columns `Jupiter Tisserand Invariant` and `Mean Motion` are highly correlated.\n",
    "    - The Columns `Epoch Osculation` and `Perihelion Time` are highly correlated.\n",
    "    - The columns `Semi Major Axis` and `Orbital Period` are highly correlated.\n",
    "    - The columns `Semi Major Axis` and `Aphelion Dist` are highly correlated.\n",
    "\n",
    "Therefore, we removed these columns from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove highly correlated columns\n",
    "cols_to_remove = [\n",
    "    'Est Dia in M(min)',\n",
    "    'Jupiter Tisserand Invariant',\n",
    "    'Epoch Osculation',\n",
    "    'Orbital Period',\n",
    "    'Aphelion Dist',\n",
    "]\n",
    "\n",
    "df.drop(cols_to_remove, axis=1, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "numerical.remove('Hazardous')\n",
    "\n",
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplots for numerical columns\n",
    "\n",
    "fig, axes = plt.subplots(math.ceil(len(numerical) / 4), 4, figsize=(25, 25))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "axes = axes.ravel()\n",
    "\n",
    "# for i, col in enumerate(numerical):\n",
    "#     sb.boxplot(x='Hazardous', y=col, data=df, ax=axes[i])\n",
    "\n",
    "for col, axis in zip(numerical, axes):\n",
    "    sb.boxplot(data=df[col], ax=axis)\n",
    "\n",
    "for i in range(len(numerical), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the outliers, we used the ```describe()``` method to get a better understanding of the data we're about to utilize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "\n",
    "for col in numerical:\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having processed and analyzed the data, we then created the training and testing datasets. We decided to utilize a 75/25 split - 75% of the dataset is used for training and the remaining 25% is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = df.drop('Hazardous', axis=1)\n",
    "hazard = df['Hazardous']\n",
    "\n",
    "(training_inputs,\n",
    "     testing_inputs,\n",
    "     training_classes,\n",
    "     testing_classes) = train_test_split(new_data, hazard, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also created a ```data_results``` function to help us analyze and compare the results for each of the classifier algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_results(testing_classes, testing_inputs, alg_class):\n",
    "    cm_display = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=confusion_matrix(testing_classes, alg_class.predict(testing_inputs))\n",
    "    )\n",
    "\n",
    "    cm_display.plot()\n",
    "    plt.xticks([0, 1], [\"False\", \"True\"])\n",
    "    plt.yticks([0, 1], [\"False\", \"True\"])\n",
    "    plt.xlabel('Predicted Hazard')\n",
    "    plt.ylabel('Actual Hazard')\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(testing_classes, alg_class.predict(testing_inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: *We utilized some default/hardcoded values for the parameters in some algorithms, like SVM or ANN; it is, however, perfectly possible to use different values for these.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_class = DecisionTreeClassifier(random_state=1)\n",
    "dt_class.fit(training_inputs, training_classes)\n",
    "\n",
    "dt_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, dt_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, dt_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_class = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn_class.fit(training_inputs, training_classes)\n",
    "\n",
    "knn_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, knn_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, knn_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_class = SVC(kernel='rbf')\n",
    "svm_class.fit(training_inputs, training_classes)\n",
    "\n",
    "svm_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, svm_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, svm_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_class = MLPClassifier(hidden_layer_sizes=(25*4, 25*2, 25), activation='logistic', solver='adam',\n",
    "                            max_iter=1000, random_state=1)\n",
    "ann_class.fit(training_inputs, training_classes)\n",
    "\n",
    "ann_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, ann_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, ann_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_class = GaussianNB()\n",
    "nb_class.fit(training_inputs, training_classes)\n",
    "\n",
    "nb_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, nb_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, nb_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_class = RandomForestClassifier(n_estimators=100)\n",
    "rf_class.fit(training_inputs, training_classes)\n",
    "\n",
    "rf_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, rf_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, rf_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was clear that the Decision Tree and Random Forest algorithms were, by far, the best models for this problem.\n",
    "\n",
    "Due to the nature of the dataset - small and unbalanced - we found that Nearest Neighbors, Support Vector Machines, Neural Networks and Naive Bayes were not suitabe for this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
