{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Asteroids as Hazardous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/nasa.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, we can find columns that are note needed since they are duplicated in different units of measure.\n",
    "We can also remove identification columns since they don't provide useful information to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    # Remove duplicated columns (same data, different units of measure)\n",
    "    \"Est Dia in KM(min)\",\n",
    "    \"Est Dia in KM(max)\",\n",
    "    \"Est Dia in Miles(min)\",\n",
    "    \"Est Dia in Miles(max)\",\n",
    "    \"Est Dia in Feet(min)\",\n",
    "    \"Est Dia in Feet(max)\",\n",
    "    \"Relative Velocity km per hr\",\n",
    "    \"Miles per hour\",\n",
    "    \"Miss Dist.(Astronomical)\",\n",
    "    \"Miss Dist.(lunar)\",\n",
    "    \"Miss Dist.(miles)\",\n",
    "\n",
    "    # Remove identification columns\n",
    "    \"Neo Reference ID\",\n",
    "    \"Name\",\n",
    "    \"Orbit ID\",\n",
    "    'Close Approach Date',\n",
    "    'Orbit Determination Date',\n",
    "]\n",
    "\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated values\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Hazardous\"] = label_encoder.fit_transform(df[\"Hazardous\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove categorical data that is not relevant to the problem (has a single value)\n",
    "categorical = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "unique_categorical = [cat for cat in categorical if df[cat].nunique() == 1]\n",
    "unique_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Orbiting Body', 'Equinox'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done the preprocessing, we can now proceed to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sb.heatmap(correlation_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the heatmap in mind we can see that:\n",
    "\n",
    "    - The columns `Est Dia in M(min)` and `Est Dia in M(max)` are highly correlated.\n",
    "    - The columns `Jupiter Tisserand Invariant` and `Mean Motion` are highly correlated.\n",
    "    - The Columns `Epoch Osculation` and `Perihelion Time` are highly correlated.\n",
    "    - The columns `Semi Major Axis` and `Orbital Period` are highly correlated.\n",
    "    - The columns `Semi Major Axis` and `Aphelion Dist` are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove highly correlated columns\n",
    "cols_to_remove = [\n",
    "    'Est Dia in M(min)',\n",
    "    'Jupiter Tisserand Invariant',\n",
    "    'Epoch Osculation',\n",
    "    'Orbital Period',\n",
    "    'Aphelion Dist',\n",
    "]\n",
    "\n",
    "df.drop(cols_to_remove, axis=1, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "numerical.remove('Hazardous')\n",
    "\n",
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplots for numerical columns\n",
    "\n",
    "fig, axes = plt.subplots(math.ceil(len(numerical) / 4), 4, figsize=(25, 25))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "axes = axes.ravel()\n",
    "\n",
    "# for i, col in enumerate(numerical):\n",
    "#     sb.boxplot(x='Hazardous', y=col, data=df, ax=axes[i])\n",
    "\n",
    "for col, axis in zip(numerical, axes):\n",
    "    sb.boxplot(data=df[col], ax=axis)\n",
    "\n",
    "for i in range(len(numerical), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "\n",
    "for col in numerical:\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = df.drop('Hazardous', axis=1)\n",
    "hazard = df['Hazardous']\n",
    "\n",
    "(training_inputs,\n",
    "     testing_inputs,\n",
    "     training_classes,\n",
    "     testing_classes) = train_test_split(new_data, hazard, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_results(testing_classes, testing_inputs, alg_class):\n",
    "    cm_display = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=confusion_matrix(testing_classes, alg_class.predict(testing_inputs))\n",
    "    )\n",
    "\n",
    "    cm_display.plot()\n",
    "    plt.xticks([0, 1], [\"False\", \"True\"])\n",
    "    plt.yticks([0, 1], [\"False\", \"True\"])\n",
    "    plt.xlabel('Predicted Hazard')\n",
    "    plt.ylabel('Actual Hazard')\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(testing_classes, alg_class.predict(testing_inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_class = DecisionTreeClassifier(random_state=1)\n",
    "dt_class.fit(training_inputs, training_classes)\n",
    "\n",
    "dt_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, dt_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, dt_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_class = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn_class.fit(training_inputs, training_classes)\n",
    "\n",
    "knn_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, knn_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, knn_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_class = SVC(kernel='rbf')\n",
    "svm_class.fit(training_inputs, training_classes)\n",
    "\n",
    "svm_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, svm_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, svm_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_class = MLPClassifier(hidden_layer_sizes=(25*4, 25*2, 25), activation='logistic', solver='adam',\n",
    "                            max_iter=1000, random_state=1)\n",
    "ann_class.fit(training_inputs, training_classes)\n",
    "\n",
    "ann_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, ann_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, ann_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_class = GaussianNB()\n",
    "nb_class.fit(training_inputs, training_classes)\n",
    "\n",
    "nb_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, nb_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, nb_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_class = RandomForestClassifier(n_estimators=100)\n",
    "rf_class.fit(training_inputs, training_classes)\n",
    "\n",
    "rf_class.score(testing_inputs, testing_classes)\n",
    "\n",
    "accuracy_score(testing_classes, rf_class.predict(testing_inputs))\n",
    "\n",
    "data_results(testing_classes, testing_inputs, rf_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is clear that decision trees and random forests are the best models for this problem.\n",
    "\n",
    "Since the dataset is small and not balanced, we found that nearest neighbors, support vector machines, neural networks and naive bayes are not good models for this problem.\n",
    "\n",
    "Data imbalance can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hazardous'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
